{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 440/540 Machine Learning in Finance: Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data files from LMS. Code/Explain your solution over this `IPython` notebook at required cells, and complete locally.\n",
    "\n",
    "To submit your assignment, in LMS, upload your solution to LMS as a single notebook with following file name format:\n",
    "\n",
    "`lastName_firstName_CourseNumber_HW3.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 440 or CS 540).\n",
    "\n",
    "Problems on homework assignments are equally weighted.\n",
    "\n",
    "Any type of plagiarism will not be tolerated. Your submitted codes will be compared with other submissions and also the codes available on internet and violations will have a penalty of -100 points. (In case of copying from\n",
    "another student both parties will get -100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:51:44.457875100Z",
     "start_time": "2023-12-16T16:51:42.085002100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries before starting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: XGBoost and Random Forest for Detecting Fraudulent Transactions\n",
    "\n",
    "In this problem, we will focus on predicting whether a transaction is a fraud or not. All transactions are provided in \"transactions.csv\". The file contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, original features cannot be provided. Features V1, V2, …, V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "Analyze the dataset. Do you think dataset is balanced? Evaluate and compare XGBoost and RandomForest algorithms without SMOTE, as well as after balancing the data via SMOTE. You can compare performance by F1 score and use 5-fold cross-validation for hyperparamter optimization.\n",
    "\n",
    "Among 4 scenarios(Random Forest, XGBoost, Random Forest + SMOTE, XGBoost + SMOTE), which performs the best? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:39:17.524117500Z",
     "start_time": "2023-12-13T10:39:15.951877900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution 1\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Class\n0    284315\n1       492\nName: count, dtype: int64"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts()\n",
    "# Dataset is not balanced. There are 492 frauds and 284315 non-fraudulent transactions."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:39:17.552123100Z",
     "start_time": "2023-12-13T10:39:17.527118500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "folds: list[tuple[pd.DataFrame, pd.DataFrame]] = []\n",
    "\n",
    "fold_length = len(df) // 5\n",
    "\n",
    "for i in range(5):\n",
    "    test_start = i * fold_length\n",
    "    test_end = (i + 1) * fold_length\n",
    "    \n",
    "    df_test = df.iloc[test_start:test_end]\n",
    "    df_train = df.drop(df_test.index)\n",
    "    \n",
    "    folds.append((df_train, df_test))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:39:17.769691400Z",
     "start_time": "2023-12-13T10:39:17.541121400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "          Time        V1        V2        V3        V4        V5        V6  \\\n56961  47694.0  1.138149 -0.698637  0.332976  0.272394 -0.629432  0.463714   \n56962  47695.0  0.017199 -0.148533 -0.095542 -0.923477  1.161514 -0.560818   \n56963  47695.0  1.157218 -0.400497  0.224997 -0.662346 -0.058859  0.701721   \n56964  47695.0  1.227454  0.052971  0.074560  1.151042 -0.047766  0.092690   \n56965  47695.0  1.228671  1.260991 -1.699439  1.450723  1.079511 -1.310171   \n\n             V7        V8        V9  ...       V21       V22       V23  \\\n56961 -0.628447  0.096643 -1.013536  ... -0.200924 -0.328477 -0.240835   \n56962  0.874059 -0.141331 -0.241034  ...  0.034412  0.099442  0.785986   \n56963 -0.468211  0.178250  0.147136  ... -0.082828 -0.319259 -0.084150   \n56964 -0.109024  0.155374  0.385462  ... -0.156081 -0.366528 -0.180845   \n56965  0.683609 -0.232911 -0.722463  ... -0.177758 -0.433050 -0.265018   \n\n            V24       V25       V26       V27       V28  Amount  Class  \n56961 -0.868956  0.483849 -0.256578  0.045728  0.034950  116.00      0  \n56962 -0.291115 -2.653623  0.148533  0.101011  0.124571   55.98      0  \n56963 -1.238330  0.039034  1.321246 -0.076822 -0.002819   69.99      0  \n56964 -0.542113  0.751469 -0.301263  0.006493 -0.003925    5.37      0  \n56965 -0.320238  0.880613 -0.291865  0.037405  0.083940    0.89      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>56961</th>\n      <td>47694.0</td>\n      <td>1.138149</td>\n      <td>-0.698637</td>\n      <td>0.332976</td>\n      <td>0.272394</td>\n      <td>-0.629432</td>\n      <td>0.463714</td>\n      <td>-0.628447</td>\n      <td>0.096643</td>\n      <td>-1.013536</td>\n      <td>...</td>\n      <td>-0.200924</td>\n      <td>-0.328477</td>\n      <td>-0.240835</td>\n      <td>-0.868956</td>\n      <td>0.483849</td>\n      <td>-0.256578</td>\n      <td>0.045728</td>\n      <td>0.034950</td>\n      <td>116.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56962</th>\n      <td>47695.0</td>\n      <td>0.017199</td>\n      <td>-0.148533</td>\n      <td>-0.095542</td>\n      <td>-0.923477</td>\n      <td>1.161514</td>\n      <td>-0.560818</td>\n      <td>0.874059</td>\n      <td>-0.141331</td>\n      <td>-0.241034</td>\n      <td>...</td>\n      <td>0.034412</td>\n      <td>0.099442</td>\n      <td>0.785986</td>\n      <td>-0.291115</td>\n      <td>-2.653623</td>\n      <td>0.148533</td>\n      <td>0.101011</td>\n      <td>0.124571</td>\n      <td>55.98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56963</th>\n      <td>47695.0</td>\n      <td>1.157218</td>\n      <td>-0.400497</td>\n      <td>0.224997</td>\n      <td>-0.662346</td>\n      <td>-0.058859</td>\n      <td>0.701721</td>\n      <td>-0.468211</td>\n      <td>0.178250</td>\n      <td>0.147136</td>\n      <td>...</td>\n      <td>-0.082828</td>\n      <td>-0.319259</td>\n      <td>-0.084150</td>\n      <td>-1.238330</td>\n      <td>0.039034</td>\n      <td>1.321246</td>\n      <td>-0.076822</td>\n      <td>-0.002819</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56964</th>\n      <td>47695.0</td>\n      <td>1.227454</td>\n      <td>0.052971</td>\n      <td>0.074560</td>\n      <td>1.151042</td>\n      <td>-0.047766</td>\n      <td>0.092690</td>\n      <td>-0.109024</td>\n      <td>0.155374</td>\n      <td>0.385462</td>\n      <td>...</td>\n      <td>-0.156081</td>\n      <td>-0.366528</td>\n      <td>-0.180845</td>\n      <td>-0.542113</td>\n      <td>0.751469</td>\n      <td>-0.301263</td>\n      <td>0.006493</td>\n      <td>-0.003925</td>\n      <td>5.37</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56965</th>\n      <td>47695.0</td>\n      <td>1.228671</td>\n      <td>1.260991</td>\n      <td>-1.699439</td>\n      <td>1.450723</td>\n      <td>1.079511</td>\n      <td>-1.310171</td>\n      <td>0.683609</td>\n      <td>-0.232911</td>\n      <td>-0.722463</td>\n      <td>...</td>\n      <td>-0.177758</td>\n      <td>-0.433050</td>\n      <td>-0.265018</td>\n      <td>-0.320238</td>\n      <td>0.880613</td>\n      <td>-0.291865</td>\n      <td>0.037405</td>\n      <td>0.083940</td>\n      <td>0.89</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[0][0].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:39:17.816277400Z",
     "start_time": "2023-12-13T10:39:17.774693800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:42<00:00, 104.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "xg_boost_f1_scores = []\n",
    "random_forest_f1_scores = []\n",
    "\n",
    "smote_xg_boost_f1_scores = []\n",
    "smote_random_forest_f1_scores = []\n",
    "\n",
    "for df_train, df_test in tqdm(folds):\n",
    "    train_X = df_train.iloc[:, :-1]\n",
    "    train_y = df_train.iloc[:, -1]\n",
    "    \n",
    "    test_X = df_test.iloc[:, :-1]\n",
    "    test_y = df_test.iloc[:, -1]\n",
    "    \n",
    "    ## XGBoost\n",
    "    xgboost = XGBClassifier()\n",
    "    xgboost: XGBClassifier\n",
    "    xgboost.fit(train_X, train_y)\n",
    "    \n",
    "    predictions_test = xgboost.predict(test_X)\n",
    "    f1s = f1_score(test_y, predictions_test)\n",
    "    xg_boost_f1_scores.append(f1s) \n",
    "    \n",
    "    ## Random Forest\n",
    "    random_forest = RandomForestClassifier(n_estimators=10)\n",
    "    random_forest: RandomForestClassifier\n",
    "    random_forest.fit(train_X, train_y)\n",
    "    \n",
    "    predictions_test = random_forest.predict(test_X)\n",
    "    f1s = f1_score(test_y, predictions_test)\n",
    "    random_forest_f1_scores.append(f1s)\n",
    "    \n",
    "    ## Smote the data\n",
    "    \n",
    "    smote_X_train, smote_y_train = SMOTE().fit_resample(train_X, train_y)\n",
    "    \n",
    "    ## XGBoost + Smote\n",
    "    xgboost = XGBClassifier()\n",
    "    xgboost: XGBClassifier\n",
    "    xgboost.fit(smote_X_train, smote_y_train)\n",
    "    \n",
    "    predictions_test = xgboost.predict(test_X)\n",
    "    f1s = f1_score(test_y, predictions_test)\n",
    "    smote_xg_boost_f1_scores.append(f1s) \n",
    "    \n",
    "    ## Random Forest + Smote\n",
    "    random_forest = RandomForestClassifier(n_estimators=10)\n",
    "    random_forest: RandomForestClassifier\n",
    "    random_forest.fit(smote_X_train, smote_y_train)\n",
    "    \n",
    "    predictions_test = random_forest.predict(test_X)\n",
    "    f1s = f1_score(test_y, predictions_test)\n",
    "    smote_random_forest_f1_scores.append(f1s)\n",
    "    \n",
    "\n",
    "xgboost_f1 = np.mean(xg_boost_f1_scores)\n",
    "random_forest_f1 = np.mean(random_forest_f1_scores)\n",
    "smote_xgboost_f1 = np.mean(smote_xg_boost_f1_scores)\n",
    "smote_random_forest_f1 = np.mean(smote_random_forest_f1_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:02:52.177501400Z",
     "start_time": "2023-12-13T10:54:09.843248900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1: 0.8217737463512647\n",
      "Random Forest F1: 0.7943355609579489\n",
      "SMOTE XGBoost F1: 0.7722751511568273\n",
      "SMOTE Random Forest F1: 0.7851228490891498\n"
     ]
    }
   ],
   "source": [
    "print(f\"XGBoost F1: {xgboost_f1}\")\n",
    "print(f\"Random Forest F1: {random_forest_f1}\")\n",
    "print(f\"SMOTE XGBoost F1: {smote_xgboost_f1}\")\n",
    "print(f\"SMOTE Random Forest F1: {smote_random_forest_f1}\")\n",
    "\n",
    "# XGBoost seems to peform better than other approaches. SMOTE does not seem to help much."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:24:44.579525Z",
     "start_time": "2023-12-13T11:24:44.563521100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: MLP for House Price Prediction\n",
    "\n",
    "Let's focus on the same Real State Price dataset from HW1 and HW2. In this problem, you are provided a single dataset \"kaggle_house.csv\" which includes both train and test sets. We will now implement four MLPs: \n",
    "\n",
    "a- MLP with 1 hidden layer with 8 units in hidden layer\n",
    "\n",
    "b- MLP with 1 hidden layer with 4 units in hidden layer\n",
    "\n",
    "c- MLP with 2 hidden layers with 4 units in both first and second layers.\n",
    "\n",
    "d- MLP with 2 hidden layers with 8 units in first layer and 4 units in the second layer.\n",
    "\n",
    "We will predict house sale price (last column) by using the following attributes: \"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\". Report the performance in terms of R2 and RMSE for the test set by applying 5-fold cross-validation.\n",
    "\n",
    "Note that you need to carefully tune learning rate and number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T12:49:46.475318400Z",
     "start_time": "2023-12-13T12:49:46.431309400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1201"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution 2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('kaggle_house.csv')\n",
    "df = df[[\"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\", \"SalePrice\"]]\n",
    "non_numeric_columns = [\"MSZoning\", \"Street\", \"LotShape\"]\n",
    "for column in non_numeric_columns:\n",
    "    df[column] = LabelEncoder().fit_transform(df[column])\n",
    "\n",
    "df = df.dropna()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:49:47.238490700Z",
     "start_time": "2023-12-13T12:49:47.221487400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "df_test = df.sample(frac=0.2)\n",
    "df_train = df.drop(df_test.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:49:47.566565500Z",
     "start_time": "2023-12-13T12:49:47.551561500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, structure_dict: dict[int, int], input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        prev_layer_units = input_dim\n",
    "        for layer, n_unit in structure_dict.items():\n",
    "            layers.append(nn.Linear(prev_layer_units, n_unit))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_layer_units = n_unit\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_units, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:49:47.929820800Z",
     "start_time": "2023-12-13T12:49:47.918818400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train(_model: MLP, X_train: torch.Tensor, y_train: torch.Tensor, X_test: torch.Tensor, y_test: torch.Tensor, learning_rate, epochs):\n",
    "    batch_size = 32\n",
    "    optimizer = torch.optim.Adam(_model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    epoch_bar = tqdm(range(epochs))\n",
    "    for epoch in epoch_bar:\n",
    "        for X, y in dataloader:\n",
    "            pred = _model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            epoch_bar.set_description(f\"Epoch {epoch}, Test Loss: {loss_function(_model(X_test), y_test)}, Train Loss: {loss_function(_model(X_train), y_train)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:56:05.408556400Z",
     "start_time": "2023-12-13T13:56:05.401554800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "folds: list[tuple[pd.DataFrame, pd.DataFrame]] = []\n",
    "\n",
    "fold_length = len(df) // 5\n",
    "\n",
    "for i in range(5):\n",
    "    test_start = i * fold_length\n",
    "    test_end = (i + 1) * fold_length\n",
    "    \n",
    "    df_test = df.iloc[test_start:test_end]\n",
    "    df_train = df.drop(df_test.index)\n",
    "    \n",
    "    folds.append((df_train, df_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:56:06.039699300Z",
     "start_time": "2023-12-13T13:56:06.006691300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:56:06.529809400Z",
     "start_time": "2023-12-13T13:56:06.526808900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def grid_search(structure):\n",
    "    epoch_lengts = [100, 500, 1000, 1500]\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    \n",
    "    params = product(epoch_lengts, learning_rates)\n",
    "    \n",
    "    results = []\n",
    "    for epoch_length, lr in params:\n",
    "        r2_scores = []\n",
    "        rmse_scores = []\n",
    "        \n",
    "        for df_train, df_test in folds:\n",
    "            X_train = torch.tensor(df_train.iloc[:, :-1].values, dtype=torch.float32, device=device)\n",
    "            y_train = torch.tensor(df_train.iloc[:, -1].values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "            X_test = torch.tensor(df_test.iloc[:, :-1].values, dtype=torch.float32, device=device)\n",
    "            y_test = torch.tensor(df_test.iloc[:, -1].values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "            \n",
    "            y_test_numpy = df_test.iloc[:, -1].values\n",
    "            \n",
    "            model = MLP(structure, X_train.shape[1]).to(device)\n",
    "            train(model, X_train, y_train, X_test, y_test, lr, epoch_length)\n",
    "            \n",
    "            prediction = model(X_test).cpu().detach().numpy()\n",
    "            rmse_scores.append(np.sqrt(mean_squared_error(y_test_numpy, prediction)))\n",
    "            r2_scores.append(r2_score(y_test_numpy, prediction))\n",
    "            \n",
    "        results.append({\n",
    "        'epoch_length': epoch_length,\n",
    "        'learning_rate': lr,\n",
    "        'r2_mean': np.mean(r2_scores),\n",
    "        'rmse_mean': np.mean(rmse_scores)\n",
    "        })\n",
    "    \n",
    "    best_result = min(results, key=lambda x: x['r2_mean'])\n",
    "    best_epochs = best_result['epoch_length']\n",
    "    best_lr = best_result['learning_rate']\n",
    "    \n",
    "    return best_epochs, best_lr\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:01:28.127449400Z",
     "start_time": "2023-12-13T14:01:28.118447500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, Test Loss: 0.0033400801476091146, Train Loss: 0.004243063274770975: 100%|██████████| 100/100 [00:05<00:00, 18.87it/s]\n",
      "Epoch 50, Test Loss: 0.0037708920426666737, Train Loss: 0.004117575008422136: 100%|██████████| 100/100 [00:05<00:00, 18.07it/s]\n",
      "Epoch 50, Test Loss: 0.004681235644966364, Train Loss: 0.004137230571359396: 100%|██████████| 100/100 [00:05<00:00, 16.73it/s]\n",
      "Epoch 50, Test Loss: 0.0035494451876729727, Train Loss: 0.00429045595228672: 100%|██████████| 100/100 [00:05<00:00, 19.76it/s]\n",
      "Epoch 50, Test Loss: 0.006742347031831741, Train Loss: 0.004356927238404751: 100%|██████████| 100/100 [00:05<00:00, 17.70it/s]\n",
      "Epoch 50, Test Loss: 0.004444521386176348, Train Loss: 0.0049465112388134: 100%|██████████| 100/100 [00:09<00:00, 10.96it/s]\n",
      "Epoch 50, Test Loss: 0.005799348931759596, Train Loss: 0.003715687897056341: 100%|██████████| 100/100 [00:05<00:00, 17.77it/s]\n",
      "Epoch 50, Test Loss: 0.0064147282391786575, Train Loss: 0.005354071501642466: 100%|██████████| 100/100 [00:06<00:00, 14.93it/s]\n",
      "Epoch 50, Test Loss: 0.005149925593286753, Train Loss: 0.005457674153149128: 100%|██████████| 100/100 [00:05<00:00, 17.31it/s]\n",
      "Epoch 50, Test Loss: 0.0062256865203380585, Train Loss: 0.0037345262244343758: 100%|██████████| 100/100 [00:05<00:00, 17.43it/s]\n",
      "Epoch 50, Test Loss: 0.01217220351099968, Train Loss: 0.013711569830775261: 100%|██████████| 100/100 [00:06<00:00, 15.18it/s]\n",
      "Epoch 50, Test Loss: 0.013572361320257187, Train Loss: 0.013422626070678234: 100%|██████████| 100/100 [00:06<00:00, 15.86it/s]\n",
      "Epoch 50, Test Loss: 0.016752192750573158, Train Loss: 0.013031640090048313: 100%|██████████| 100/100 [00:05<00:00, 17.60it/s]\n",
      "Epoch 50, Test Loss: 0.0031012953259050846, Train Loss: 0.004296764265745878: 100%|██████████| 100/100 [00:05<00:00, 18.31it/s]\n",
      "Epoch 50, Test Loss: 0.014396332204341888, Train Loss: 0.014384496957063675: 100%|██████████| 100/100 [00:06<00:00, 15.11it/s]\n",
      "Epoch 450, Test Loss: 0.002443913836032152, Train Loss: 0.0024749061558395624: 100%|██████████| 500/500 [00:29<00:00, 17.06it/s] \n",
      "Epoch 450, Test Loss: 0.004220499657094479, Train Loss: 0.003130912082269788: 100%|██████████| 500/500 [00:29<00:00, 17.13it/s]  \n",
      "Epoch 450, Test Loss: 0.003830116242170334, Train Loss: 0.002478791633620858: 100%|██████████| 500/500 [00:29<00:00, 17.09it/s]  \n",
      "Epoch 450, Test Loss: 0.0031775538809597492, Train Loss: 0.003366971155628562: 100%|██████████| 500/500 [00:25<00:00, 19.64it/s] \n",
      "Epoch 450, Test Loss: 0.007993945851922035, Train Loss: 0.002544092247262597: 100%|██████████| 500/500 [00:26<00:00, 18.96it/s] \n",
      "Epoch 450, Test Loss: 0.007379577029496431, Train Loss: 0.008474372327327728: 100%|██████████| 500/500 [00:25<00:00, 19.61it/s]  \n",
      "Epoch 450, Test Loss: 0.004976887721568346, Train Loss: 0.0020146076567471027: 100%|██████████| 500/500 [00:26<00:00, 19.15it/s]\n",
      "Epoch 450, Test Loss: 0.0041421870701014996, Train Loss: 0.004305428825318813: 100%|██████████| 500/500 [00:26<00:00, 18.93it/s]\n",
      "Epoch 450, Test Loss: 0.0032501411624252796, Train Loss: 0.003219211706891656: 100%|██████████| 500/500 [00:25<00:00, 19.41it/s] \n",
      "Epoch 450, Test Loss: 0.008960399776697159, Train Loss: 0.0029238909482955933: 100%|██████████| 500/500 [00:25<00:00, 19.97it/s]\n",
      "Epoch 450, Test Loss: 0.012539778836071491, Train Loss: 0.014181915670633316: 100%|██████████| 500/500 [00:25<00:00, 19.64it/s]\n",
      "Epoch 450, Test Loss: 0.015714291483163834, Train Loss: 0.015938248485326767: 100%|██████████| 500/500 [00:25<00:00, 19.85it/s]\n",
      "Epoch 450, Test Loss: 0.0168595090508461, Train Loss: 0.0132176848128438: 100%|██████████| 500/500 [00:28<00:00, 17.84it/s]    \n",
      "Epoch 450, Test Loss: 0.01115538738667965, Train Loss: 0.014757812954485416: 100%|██████████| 500/500 [00:29<00:00, 17.15it/s] \n",
      "Epoch 450, Test Loss: 0.005089093465358019, Train Loss: 0.003429784206673503: 100%|██████████| 500/500 [00:28<00:00, 17.83it/s]\n",
      "Epoch 950, Test Loss: 0.0030346207786351442, Train Loss: 0.003219235222786665: 100%|██████████| 1000/1000 [00:58<00:00, 17.23it/s]\n",
      "Epoch 950, Test Loss: 0.0035362825728952885, Train Loss: 0.0024715999606996775: 100%|██████████| 1000/1000 [00:57<00:00, 17.26it/s]\n",
      "Epoch 950, Test Loss: 0.004191169515252113, Train Loss: 0.003395860781893134: 100%|██████████| 1000/1000 [00:58<00:00, 17.03it/s] \n",
      "Epoch 950, Test Loss: 0.002523575909435749, Train Loss: 0.0024499755818396807: 100%|██████████| 1000/1000 [00:57<00:00, 17.39it/s]\n",
      "Epoch 950, Test Loss: 0.003632825333625078, Train Loss: 0.001918759080581367: 100%|██████████| 1000/1000 [00:58<00:00, 17.12it/s] \n",
      "Epoch 950, Test Loss: 0.004319025669246912, Train Loss: 0.004884183406829834: 100%|██████████| 1000/1000 [00:58<00:00, 17.13it/s] \n",
      "Epoch 950, Test Loss: 0.004430537112057209, Train Loss: 0.00342614040710032: 100%|██████████| 1000/1000 [00:59<00:00, 16.89it/s] \n",
      "Epoch 950, Test Loss: 0.004673211835324764, Train Loss: 0.004149763844907284: 100%|██████████| 1000/1000 [00:59<00:00, 16.84it/s] \n",
      "Epoch 950, Test Loss: 0.003502089064568281, Train Loss: 0.004382566083222628: 100%|██████████| 1000/1000 [00:58<00:00, 17.13it/s] \n",
      "Epoch 950, Test Loss: 0.004916828125715256, Train Loss: 0.0038063745014369488: 100%|██████████| 1000/1000 [00:58<00:00, 17.11it/s]\n",
      "Epoch 950, Test Loss: 0.013534752652049065, Train Loss: 0.014875421300530434: 100%|██████████| 1000/1000 [00:58<00:00, 17.21it/s]\n",
      "Epoch 950, Test Loss: 0.014222420752048492, Train Loss: 0.013813384808599949: 100%|██████████| 1000/1000 [00:59<00:00, 16.90it/s]\n",
      "Epoch 950, Test Loss: 0.016766127198934555, Train Loss: 0.01305727194994688: 100%|██████████| 1000/1000 [00:58<00:00, 17.21it/s]\n",
      "Epoch 950, Test Loss: 0.012152576819062233, Train Loss: 0.016426945105195045: 100%|██████████| 1000/1000 [00:58<00:00, 16.98it/s]\n",
      "Epoch 950, Test Loss: 0.01602095179259777, Train Loss: 0.016045263037085533: 100%|██████████| 1000/1000 [00:57<00:00, 17.30it/s]\n",
      "Epoch 1450, Test Loss: 0.0020548864267766476, Train Loss: 0.0023699242155998945: 100%|██████████| 1500/1500 [01:26<00:00, 17.26it/s]\n",
      "Epoch 1450, Test Loss: 0.004939511884003878, Train Loss: 0.0018224481027573347: 100%|██████████| 1500/1500 [01:25<00:00, 17.47it/s] \n",
      "Epoch 1450, Test Loss: 0.0025990011636167765, Train Loss: 0.002222710754722357: 100%|██████████| 1500/1500 [01:22<00:00, 18.17it/s] \n",
      "Epoch 1450, Test Loss: 0.0030444515869021416, Train Loss: 0.0034117908217012882: 100%|██████████| 1500/1500 [01:09<00:00, 21.54it/s]\n",
      "Epoch 1450, Test Loss: 0.004825232550501823, Train Loss: 0.002130373613908887: 100%|██████████| 1500/1500 [01:04<00:00, 23.09it/s]  \n",
      "Epoch 1450, Test Loss: 0.003086417680606246, Train Loss: 0.004109756555408239: 100%|██████████| 1500/1500 [01:08<00:00, 21.85it/s]  \n",
      "Epoch 1450, Test Loss: 0.004986075218766928, Train Loss: 0.003683356801047921: 100%|██████████| 1500/1500 [01:06<00:00, 22.61it/s] \n",
      "Epoch 1450, Test Loss: 0.0038346147630363703, Train Loss: 0.003936458379030228: 100%|██████████| 1500/1500 [01:07<00:00, 22.10it/s]\n",
      "Epoch 1450, Test Loss: 0.003600986674427986, Train Loss: 0.004538276232779026: 100%|██████████| 1500/1500 [01:05<00:00, 22.87it/s] \n",
      "Epoch 1450, Test Loss: 0.007741130888462067, Train Loss: 0.0034952687565237284: 100%|██████████| 1500/1500 [01:05<00:00, 22.77it/s]\n",
      "Epoch 1450, Test Loss: 0.01250551175326109, Train Loss: 0.01414272841066122: 100%|██████████| 1500/1500 [01:04<00:00, 23.36it/s]  \n",
      "Epoch 1450, Test Loss: 0.013997841626405716, Train Loss: 0.013630907982587814: 100%|██████████| 1500/1500 [01:05<00:00, 22.89it/s]\n",
      "Epoch 1450, Test Loss: 0.018000749871134758, Train Loss: 0.013318726792931557: 100%|██████████| 1500/1500 [01:07<00:00, 22.24it/s]\n",
      "Epoch 1450, Test Loss: 0.010998481884598732, Train Loss: 0.014189799316227436: 100%|██████████| 1500/1500 [01:11<00:00, 20.86it/s]\n",
      "Epoch 1450, Test Loss: 0.015653446316719055, Train Loss: 0.015669269487261772: 100%|██████████| 1500/1500 [01:10<00:00, 21.21it/s]\n",
      "Epoch 50, Test Loss: 0.005082426592707634, Train Loss: 0.006020484957844019: 100%|██████████| 100/100 [00:04<00:00, 20.26it/s]\n",
      "Epoch 50, Test Loss: 0.004262992180883884, Train Loss: 0.004231128841638565: 100%|██████████| 100/100 [00:04<00:00, 20.80it/s]\n",
      "Epoch 50, Test Loss: 0.005500131752341986, Train Loss: 0.0039368378929793835: 100%|██████████| 100/100 [00:04<00:00, 20.09it/s]\n",
      "Epoch 50, Test Loss: 0.0035307181533426046, Train Loss: 0.003979754634201527: 100%|██████████| 100/100 [00:04<00:00, 20.89it/s]\n",
      "Epoch 50, Test Loss: 0.006547271739691496, Train Loss: 0.004102938808500767: 100%|██████████| 100/100 [00:04<00:00, 20.91it/s]\n",
      "Epoch 50, Test Loss: 0.0033520010765641928, Train Loss: 0.004318199586123228: 100%|██████████| 100/100 [00:05<00:00, 19.23it/s]\n",
      "Epoch 50, Test Loss: 0.005085801240056753, Train Loss: 0.004651022143661976: 100%|██████████| 100/100 [00:04<00:00, 20.12it/s]\n",
      "Epoch 50, Test Loss: 0.004491168074309826, Train Loss: 0.003980520647019148: 100%|██████████| 100/100 [00:04<00:00, 22.35it/s]\n",
      "Epoch 50, Test Loss: 0.003221010323613882, Train Loss: 0.003640218637883663: 100%|██████████| 100/100 [00:04<00:00, 21.69it/s]\n",
      "Epoch 50, Test Loss: 0.006230358965694904, Train Loss: 0.0035825020167976618: 100%|██████████| 100/100 [00:04<00:00, 23.30it/s]\n",
      "Epoch 50, Test Loss: 0.012527063488960266, Train Loss: 0.013964890502393246: 100%|██████████| 100/100 [00:04<00:00, 22.13it/s]\n",
      "Epoch 50, Test Loss: 0.01511454489082098, Train Loss: 0.01458563283085823: 100%|██████████| 100/100 [00:04<00:00, 20.04it/s]\n",
      "Epoch 50, Test Loss: 0.007920210249722004, Train Loss: 0.009361423552036285: 100%|██████████| 100/100 [00:04<00:00, 21.54it/s]\n",
      "Epoch 50, Test Loss: 0.003556298790499568, Train Loss: 0.004686326254159212: 100%|██████████| 100/100 [00:04<00:00, 21.53it/s]\n",
      "Epoch 50, Test Loss: 0.008991888724267483, Train Loss: 0.00845826230943203: 100%|██████████| 100/100 [00:04<00:00, 21.00it/s]\n",
      "Epoch 450, Test Loss: 0.002871493110433221, Train Loss: 0.003229383612051606: 100%|██████████| 500/500 [00:24<00:00, 20.41it/s]  \n",
      "Epoch 450, Test Loss: 0.00407209899276495, Train Loss: 0.004047717899084091: 100%|██████████| 500/500 [00:24<00:00, 20.78it/s] \n",
      "Epoch 450, Test Loss: 0.003398231929168105, Train Loss: 0.002870142227038741: 100%|██████████| 500/500 [00:23<00:00, 20.85it/s]  \n",
      "Epoch 450, Test Loss: 0.0031774325761944056, Train Loss: 0.004249005112797022: 100%|██████████| 500/500 [00:24<00:00, 20.39it/s] \n",
      "Epoch 450, Test Loss: 0.005071979481726885, Train Loss: 0.0034614484757184982: 100%|██████████| 500/500 [00:24<00:00, 20.29it/s] \n",
      "Epoch 450, Test Loss: 0.003255985677242279, Train Loss: 0.0035531525500118732: 100%|██████████| 500/500 [00:23<00:00, 21.71it/s] \n",
      "Epoch 450, Test Loss: 0.005781831685453653, Train Loss: 0.00309479096904397: 100%|██████████| 500/500 [00:25<00:00, 19.60it/s]  \n",
      "Epoch 450, Test Loss: 0.004114465322345495, Train Loss: 0.0029415488243103027: 100%|██████████| 500/500 [00:23<00:00, 20.99it/s]\n",
      "Epoch 450, Test Loss: 0.0032013365998864174, Train Loss: 0.004187749698758125: 100%|██████████| 500/500 [00:22<00:00, 22.48it/s] \n",
      "Epoch 450, Test Loss: 0.005927097052335739, Train Loss: 0.0023630214855074883: 100%|██████████| 500/500 [00:24<00:00, 20.76it/s] \n",
      "Epoch 450, Test Loss: 0.01295256707817316, Train Loss: 0.014641805551946163: 100%|██████████| 500/500 [00:24<00:00, 20.44it/s] \n",
      "Epoch 450, Test Loss: 0.014101134613156319, Train Loss: 0.013713756576180458: 100%|██████████| 500/500 [00:22<00:00, 22.20it/s]\n",
      "Epoch 450, Test Loss: 0.004866943694651127, Train Loss: 0.0036409092135727406: 100%|██████████| 500/500 [00:21<00:00, 22.81it/s] \n",
      "Epoch 450, Test Loss: 0.015238200314342976, Train Loss: 0.01637982949614525: 100%|██████████| 500/500 [00:21<00:00, 23.34it/s] \n",
      "Epoch 450, Test Loss: 0.014277830719947815, Train Loss: 0.014262383803725243: 100%|██████████| 500/500 [00:23<00:00, 21.55it/s]\n",
      "Epoch 950, Test Loss: 0.003017919370904565, Train Loss: 0.003496968187391758: 100%|██████████| 1000/1000 [00:43<00:00, 22.78it/s] \n",
      "Epoch 950, Test Loss: 0.0037816509138792753, Train Loss: 0.0033495414536446333: 100%|██████████| 1000/1000 [00:43<00:00, 23.17it/s]\n",
      "Epoch 950, Test Loss: 0.004213741049170494, Train Loss: 0.0039998446591198444: 100%|██████████| 1000/1000 [00:43<00:00, 22.96it/s]\n",
      "Epoch 950, Test Loss: 0.0023531895130872726, Train Loss: 0.0028422644827514887: 100%|██████████| 1000/1000 [00:42<00:00, 23.45it/s]\n",
      "Epoch 950, Test Loss: 0.004801159258931875, Train Loss: 0.002281679306179285: 100%|██████████| 1000/1000 [00:42<00:00, 23.36it/s]\n",
      "Epoch 950, Test Loss: 0.0031368553172796965, Train Loss: 0.004109169822186232: 100%|██████████| 1000/1000 [00:42<00:00, 23.39it/s]\n",
      "Epoch 950, Test Loss: 0.003387720324099064, Train Loss: 0.002847441704943776: 100%|██████████| 1000/1000 [00:43<00:00, 22.85it/s]\n",
      "Epoch 950, Test Loss: 0.004521338269114494, Train Loss: 0.0033691658172756433: 100%|██████████| 1000/1000 [00:43<00:00, 22.73it/s]\n",
      "Epoch 950, Test Loss: 0.002994205802679062, Train Loss: 0.00359952705912292: 100%|██████████| 1000/1000 [00:43<00:00, 23.21it/s]  \n",
      "Epoch 950, Test Loss: 0.013460354879498482, Train Loss: 0.013388215564191341: 100%|██████████| 1000/1000 [00:44<00:00, 22.64it/s]\n",
      "Epoch 950, Test Loss: 0.014982879161834717, Train Loss: 0.016236992552876472: 100%|██████████| 1000/1000 [00:43<00:00, 23.09it/s]\n",
      "Epoch 950, Test Loss: 0.018739180639386177, Train Loss: 0.017925161868333817: 100%|██████████| 1000/1000 [00:43<00:00, 22.92it/s]\n",
      "Epoch 950, Test Loss: 0.01747353933751583, Train Loss: 0.01294856145977974: 100%|██████████| 1000/1000 [00:42<00:00, 23.27it/s] \n",
      "Epoch 950, Test Loss: 0.011401734314858913, Train Loss: 0.013966976664960384: 100%|██████████| 1000/1000 [00:43<00:00, 22.82it/s]\n",
      "Epoch 950, Test Loss: 0.01420275866985321, Train Loss: 0.014085778966546059: 100%|██████████| 1000/1000 [00:44<00:00, 22.58it/s]\n",
      "Epoch 1450, Test Loss: 0.0028921531047672033, Train Loss: 0.0034573578741401434: 100%|██████████| 1500/1500 [01:07<00:00, 22.36it/s]\n",
      "Epoch 1450, Test Loss: 0.005371352192014456, Train Loss: 0.0028388211503624916: 100%|██████████| 1500/1500 [01:10<00:00, 21.28it/s] \n",
      "Epoch 1450, Test Loss: 0.003648465033620596, Train Loss: 0.0036853619385510683: 100%|██████████| 1500/1500 [01:08<00:00, 21.84it/s] \n",
      "Epoch 1450, Test Loss: 0.003657836699858308, Train Loss: 0.003367943223565817: 100%|██████████| 1500/1500 [01:08<00:00, 21.97it/s]  \n",
      "Epoch 1450, Test Loss: 0.006297863554209471, Train Loss: 0.0021789998281747103: 100%|██████████| 1500/1500 [01:07<00:00, 22.06it/s]\n",
      "Epoch 1450, Test Loss: 0.003354819258674979, Train Loss: 0.0037677919026464224: 100%|██████████| 1500/1500 [01:06<00:00, 22.53it/s] \n",
      "Epoch 1450, Test Loss: 0.004582131281495094, Train Loss: 0.003935168497264385: 100%|██████████| 1500/1500 [01:04<00:00, 23.19it/s] \n",
      "Epoch 1450, Test Loss: 0.0038411193527281284, Train Loss: 0.003434773301705718: 100%|██████████| 1500/1500 [01:05<00:00, 22.73it/s] \n",
      "Epoch 1450, Test Loss: 0.003301497781649232, Train Loss: 0.0033779265359044075: 100%|██████████| 1500/1500 [01:05<00:00, 22.96it/s] \n",
      "Epoch 1450, Test Loss: 0.0031774556264281273, Train Loss: 0.002197377849370241: 100%|██████████| 1500/1500 [01:05<00:00, 22.98it/s] \n",
      "Epoch 1450, Test Loss: 0.013022507540881634, Train Loss: 0.014404849149286747: 100%|██████████| 1500/1500 [01:06<00:00, 22.59it/s]\n",
      "Epoch 1450, Test Loss: 0.013691677711904049, Train Loss: 0.01360998209565878: 100%|██████████| 1500/1500 [01:04<00:00, 23.10it/s] \n",
      "Epoch 1450, Test Loss: 0.016628125682473183, Train Loss: 0.012625440955162048: 100%|██████████| 1500/1500 [01:06<00:00, 22.61it/s]\n",
      "Epoch 1450, Test Loss: 0.011203648522496223, Train Loss: 0.01395090389996767: 100%|██████████| 1500/1500 [01:05<00:00, 22.81it/s] \n",
      "Epoch 1450, Test Loss: 0.007558414712548256, Train Loss: 0.005492233671247959: 100%|██████████| 1500/1500 [01:06<00:00, 22.71it/s]  \n",
      "Epoch 50, Test Loss: 0.004310587886720896, Train Loss: 0.005462219938635826: 100%|██████████| 100/100 [00:05<00:00, 19.84it/s]\n",
      "Epoch 50, Test Loss: 0.0034134413581341505, Train Loss: 0.0037081092596054077: 100%|██████████| 100/100 [00:04<00:00, 20.37it/s]\n",
      "Epoch 50, Test Loss: 0.006004130933433771, Train Loss: 0.0037588351406157017: 100%|██████████| 100/100 [00:04<00:00, 21.14it/s]\n",
      "Epoch 50, Test Loss: 0.002989542204886675, Train Loss: 0.003682297421619296: 100%|██████████| 100/100 [00:05<00:00, 19.90it/s]\n",
      "Epoch 50, Test Loss: 0.005503260530531406, Train Loss: 0.004020904190838337: 100%|██████████| 100/100 [00:05<00:00, 18.59it/s]\n",
      "Epoch 50, Test Loss: 0.012172346003353596, Train Loss: 0.013713476248085499: 100%|██████████| 100/100 [00:05<00:00, 19.25it/s]\n",
      "Epoch 50, Test Loss: 0.003221554681658745, Train Loss: 0.0024633994325995445: 100%|██████████| 100/100 [00:05<00:00, 18.38it/s]\n",
      "Epoch 50, Test Loss: 0.00329141435213387, Train Loss: 0.0027095715049654245: 100%|██████████| 100/100 [00:05<00:00, 19.30it/s]\n",
      "Epoch 50, Test Loss: 0.0025661946274340153, Train Loss: 0.003582816105335951: 100%|██████████| 100/100 [00:05<00:00, 19.53it/s]\n",
      "Epoch 50, Test Loss: 0.0045860810205340385, Train Loss: 0.002798871835693717: 100%|██████████| 100/100 [00:05<00:00, 18.53it/s]\n",
      "Epoch 50, Test Loss: 0.0027959176804870367, Train Loss: 0.003260829020291567: 100%|██████████| 100/100 [00:04<00:00, 20.90it/s]\n",
      "Epoch 50, Test Loss: 0.016674784943461418, Train Loss: 0.01698106899857521: 100%|██████████| 100/100 [00:04<00:00, 20.13it/s]\n",
      "Epoch 50, Test Loss: 0.0052739037200808525, Train Loss: 0.003535304917022586: 100%|██████████| 100/100 [00:05<00:00, 19.44it/s]\n",
      "Epoch 50, Test Loss: 0.01122676394879818, Train Loss: 0.013949191197752953: 100%|██████████| 100/100 [00:05<00:00, 19.00it/s]\n",
      "Epoch 50, Test Loss: 0.0053609698079526424, Train Loss: 0.0033635639119893312: 100%|██████████| 100/100 [00:06<00:00, 16.50it/s]\n",
      "Epoch 450, Test Loss: 0.0030664827208966017, Train Loss: 0.0035632753279060125: 100%|██████████| 500/500 [00:25<00:00, 19.46it/s]\n",
      "Epoch 450, Test Loss: 0.0033105965703725815, Train Loss: 0.0027601325418800116: 100%|██████████| 500/500 [00:26<00:00, 18.81it/s]\n",
      "Epoch 450, Test Loss: 0.0032924439292401075, Train Loss: 0.002889835275709629: 100%|██████████| 500/500 [00:27<00:00, 18.40it/s] \n",
      "Epoch 450, Test Loss: 0.0035175937227904797, Train Loss: 0.0033139598090201616: 100%|██████████| 500/500 [00:28<00:00, 17.55it/s]\n",
      "Epoch 450, Test Loss: 0.00742438156157732, Train Loss: 0.0028730512131005526: 100%|██████████| 500/500 [00:34<00:00, 14.43it/s] \n",
      "Epoch 450, Test Loss: 0.0031100413762032986, Train Loss: 0.004204330500215292: 100%|██████████| 500/500 [00:33<00:00, 14.95it/s] \n",
      "Epoch 450, Test Loss: 0.003167136339470744, Train Loss: 0.0020434188190847635: 100%|██████████| 500/500 [00:29<00:00, 16.78it/s] \n",
      "Epoch 450, Test Loss: 0.005075514316558838, Train Loss: 0.003781160106882453: 100%|██████████| 500/500 [00:30<00:00, 16.55it/s]  \n",
      "Epoch 450, Test Loss: 0.01166782807558775, Train Loss: 0.014049999415874481: 100%|██████████| 500/500 [00:30<00:00, 16.40it/s] \n",
      "Epoch 450, Test Loss: 0.00802235584706068, Train Loss: 0.0026563983410596848: 100%|██████████| 500/500 [00:30<00:00, 16.59it/s] \n",
      "Epoch 450, Test Loss: 0.012573109939694405, Train Loss: 0.014219818636775017: 100%|██████████| 500/500 [00:30<00:00, 16.55it/s]\n",
      "Epoch 450, Test Loss: 0.013645363971590996, Train Loss: 0.013380303978919983: 100%|██████████| 500/500 [00:31<00:00, 16.01it/s]\n",
      "Epoch 450, Test Loss: 0.019199475646018982, Train Loss: 0.014247979037463665: 100%|██████████| 500/500 [00:30<00:00, 16.54it/s]\n",
      "Epoch 450, Test Loss: 0.011192534118890762, Train Loss: 0.013952224515378475: 100%|██████████| 500/500 [00:29<00:00, 17.12it/s]\n",
      "Epoch 450, Test Loss: 0.013453678227961063, Train Loss: 0.013386835344135761: 100%|██████████| 500/500 [00:29<00:00, 16.99it/s]\n",
      "Epoch 950, Test Loss: 0.0020608885679394007, Train Loss: 0.0017343990039080381: 100%|██████████| 1000/1000 [00:58<00:00, 17.13it/s]\n",
      "Epoch 950, Test Loss: 0.004171755164861679, Train Loss: 0.003161179833114147: 100%|██████████| 1000/1000 [00:57<00:00, 17.40it/s] \n",
      "Epoch 950, Test Loss: 0.0030825992580503225, Train Loss: 0.0024998951703310013: 100%|██████████| 1000/1000 [00:57<00:00, 17.36it/s]\n",
      "Epoch 950, Test Loss: 0.0027444513980299234, Train Loss: 0.002111776964738965: 100%|██████████| 1000/1000 [01:00<00:00, 16.62it/s]\n",
      "Epoch 950, Test Loss: 0.0035407368559390306, Train Loss: 0.002724760677665472: 100%|██████████| 1000/1000 [00:59<00:00, 16.94it/s]\n",
      "Epoch 950, Test Loss: 0.002847563475370407, Train Loss: 0.0029855237808078527: 100%|██████████| 1000/1000 [00:58<00:00, 16.99it/s]\n",
      "Epoch 950, Test Loss: 0.004204295575618744, Train Loss: 0.001922795781865716: 100%|██████████| 1000/1000 [00:59<00:00, 16.67it/s] \n",
      "Epoch 950, Test Loss: 0.0032998286187648773, Train Loss: 0.0028053675778210163: 100%|██████████| 1000/1000 [01:00<00:00, 16.65it/s]\n",
      "Epoch 950, Test Loss: 0.0025922097265720367, Train Loss: 0.0025780631694942713: 100%|██████████| 1000/1000 [01:00<00:00, 16.56it/s]\n",
      "Epoch 950, Test Loss: 0.003849764820188284, Train Loss: 0.002585803624242544: 100%|██████████| 1000/1000 [00:59<00:00, 16.70it/s] \n",
      "Epoch 950, Test Loss: 0.013284863904118538, Train Loss: 0.015003245323896408: 100%|██████████| 1000/1000 [00:59<00:00, 16.75it/s]\n",
      "Epoch 950, Test Loss: 0.013708039186894894, Train Loss: 0.013417554087936878: 100%|██████████| 1000/1000 [01:00<00:00, 16.66it/s]\n",
      "Epoch 950, Test Loss: 0.017320089042186737, Train Loss: 0.012849612161517143: 100%|██████████| 1000/1000 [00:58<00:00, 17.18it/s]\n",
      "Epoch 950, Test Loss: 0.012613793835043907, Train Loss: 0.014543246477842331: 100%|██████████| 1000/1000 [01:00<00:00, 16.57it/s]\n",
      "Epoch 950, Test Loss: 0.01406877115368843, Train Loss: 0.014046251773834229: 100%|██████████| 1000/1000 [00:55<00:00, 17.88it/s]\n",
      "Epoch 1450, Test Loss: 0.0020465473644435406, Train Loss: 0.001986467046663165: 100%|██████████| 1500/1500 [01:24<00:00, 17.81it/s] \n",
      "Epoch 1450, Test Loss: 0.0037532008718699217, Train Loss: 0.001964274561032653: 100%|██████████| 1500/1500 [01:24<00:00, 17.79it/s] \n",
      "Epoch 1450, Test Loss: 0.0024544859770685434, Train Loss: 0.0021464943420141935: 100%|██████████| 1500/1500 [01:24<00:00, 17.83it/s]\n",
      "Epoch 1450, Test Loss: 0.003199737286195159, Train Loss: 0.0038033591117709875: 100%|██████████| 1500/1500 [01:24<00:00, 17.79it/s] \n",
      "Epoch 1450, Test Loss: 0.004615692887455225, Train Loss: 0.0033069229684770107: 100%|██████████| 1500/1500 [01:22<00:00, 18.18it/s]\n",
      "Epoch 1450, Test Loss: 0.0025018667802214622, Train Loss: 0.0023801729548722506: 100%|██████████| 1500/1500 [01:24<00:00, 17.78it/s]\n",
      "Epoch 1450, Test Loss: 0.003671745304018259, Train Loss: 0.0035555216018110514: 100%|██████████| 1500/1500 [01:23<00:00, 17.93it/s] \n",
      "Epoch 1450, Test Loss: 0.0167960487306118, Train Loss: 0.01258646510541439: 100%|██████████| 1500/1500 [01:24<00:00, 17.84it/s]   \n",
      "Epoch 1450, Test Loss: 0.011069228872656822, Train Loss: 0.014003363437950611: 100%|██████████| 1500/1500 [01:24<00:00, 17.79it/s]\n",
      "Epoch 1450, Test Loss: 0.013527525588870049, Train Loss: 0.013444528914988041: 100%|██████████| 1500/1500 [01:23<00:00, 17.87it/s]\n",
      "Epoch 1450, Test Loss: 0.01232227124273777, Train Loss: 0.013795506209135056: 100%|██████████| 1500/1500 [01:23<00:00, 17.90it/s] \n",
      "Epoch 1450, Test Loss: 0.013602121733129025, Train Loss: 0.01347733847796917: 100%|██████████| 1500/1500 [01:27<00:00, 17.08it/s] \n",
      "Epoch 1450, Test Loss: 0.016954487189650536, Train Loss: 0.012645579874515533: 100%|██████████| 1500/1500 [01:31<00:00, 16.34it/s]\n",
      "Epoch 1450, Test Loss: 0.012331297621130943, Train Loss: 0.014377199113368988: 100%|██████████| 1500/1500 [01:31<00:00, 16.39it/s]\n",
      "Epoch 1450, Test Loss: 0.015450341627001762, Train Loss: 0.015302015468478203: 100%|██████████| 1500/1500 [01:36<00:00, 15.51it/s]\n",
      "Epoch 50, Test Loss: 0.012176125310361385, Train Loss: 0.013704334385693073: 100%|██████████| 100/100 [00:05<00:00, 17.06it/s]\n",
      "Epoch 50, Test Loss: 0.002600987907499075, Train Loss: 0.0028371489606797695: 100%|██████████| 100/100 [00:05<00:00, 18.34it/s]\n",
      "Epoch 50, Test Loss: 0.00573623226955533, Train Loss: 0.004532752092927694: 100%|██████████| 100/100 [00:05<00:00, 18.35it/s]\n",
      "Epoch 50, Test Loss: 0.0030628761742264032, Train Loss: 0.003681848756968975: 100%|██████████| 100/100 [00:05<00:00, 17.95it/s]\n",
      "Epoch 50, Test Loss: 0.005849878303706646, Train Loss: 0.002856421982869506: 100%|██████████| 100/100 [00:06<00:00, 16.52it/s]\n",
      "Epoch 50, Test Loss: 0.0029323450289666653, Train Loss: 0.003437667852267623: 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
      "Epoch 50, Test Loss: 0.004780130926519632, Train Loss: 0.0031875723507255316: 100%|██████████| 100/100 [00:05<00:00, 18.56it/s]\n",
      "Epoch 50, Test Loss: 0.003468564245849848, Train Loss: 0.0036088612396270037: 100%|██████████| 100/100 [00:05<00:00, 19.02it/s]\n",
      "Epoch 50, Test Loss: 0.002587710740044713, Train Loss: 0.0038131496403366327: 100%|██████████| 100/100 [00:05<00:00, 19.07it/s]\n",
      "Epoch 50, Test Loss: 0.004226151388138533, Train Loss: 0.0023505513090640306: 100%|██████████| 100/100 [00:05<00:00, 17.38it/s]\n",
      "Epoch 50, Test Loss: 0.012869800440967083, Train Loss: 0.014266912825405598: 100%|██████████| 100/100 [00:05<00:00, 18.29it/s]\n",
      "Epoch 50, Test Loss: 0.01445708516985178, Train Loss: 0.014536849223077297: 100%|██████████| 100/100 [00:05<00:00, 18.39it/s]\n",
      "Epoch 50, Test Loss: 0.01692609116435051, Train Loss: 0.012632880359888077: 100%|██████████| 100/100 [00:05<00:00, 18.56it/s]\n",
      "Epoch 50, Test Loss: 0.006160053890198469, Train Loss: 0.006843927316367626: 100%|██████████| 100/100 [00:05<00:00, 18.63it/s]\n",
      "Epoch 50, Test Loss: 0.009344755671918392, Train Loss: 0.0071401274763047695: 100%|██████████| 100/100 [00:05<00:00, 18.10it/s]\n",
      "Epoch 450, Test Loss: 0.002010753145441413, Train Loss: 0.00247307401150465: 100%|██████████| 500/500 [00:27<00:00, 18.42it/s]   \n",
      "Epoch 450, Test Loss: 0.003909894730895758, Train Loss: 0.00284147122874856: 100%|██████████| 500/500 [00:29<00:00, 16.79it/s]   \n",
      "Epoch 450, Test Loss: 0.0027843217831104994, Train Loss: 0.001982631627470255: 100%|██████████| 500/500 [00:30<00:00, 16.28it/s] \n",
      "Epoch 450, Test Loss: 0.0019337184494361281, Train Loss: 0.0020274522248655558: 100%|██████████| 500/500 [00:28<00:00, 17.42it/s]\n",
      "Epoch 450, Test Loss: 0.00349495280534029, Train Loss: 0.0019693593494594097: 100%|██████████| 500/500 [00:28<00:00, 17.46it/s]  \n",
      "Epoch 450, Test Loss: 0.0028821409214287996, Train Loss: 0.0034665309358388186: 100%|██████████| 500/500 [00:28<00:00, 17.43it/s]\n",
      "Epoch 450, Test Loss: 0.003883128985762596, Train Loss: 0.002355424454435706: 100%|██████████| 500/500 [00:27<00:00, 17.98it/s] \n",
      "Epoch 450, Test Loss: 0.002490783343091607, Train Loss: 0.0019112599547952414: 100%|██████████| 500/500 [00:27<00:00, 18.21it/s] \n",
      "Epoch 450, Test Loss: 0.011224057525396347, Train Loss: 0.013949325308203697: 100%|██████████| 500/500 [00:27<00:00, 17.86it/s]\n",
      "Epoch 450, Test Loss: 0.007215327583253384, Train Loss: 0.002759594703093171: 100%|██████████| 500/500 [00:28<00:00, 17.44it/s]  \n",
      "Epoch 450, Test Loss: 0.012430616654455662, Train Loss: 0.014056107960641384: 100%|██████████| 500/500 [00:28<00:00, 17.54it/s]\n",
      "Epoch 450, Test Loss: 0.014714374206960201, Train Loss: 0.014233210124075413: 100%|██████████| 500/500 [00:30<00:00, 16.62it/s]\n",
      "Epoch 450, Test Loss: 0.018414752557873726, Train Loss: 0.013629800640046597: 100%|██████████| 500/500 [00:31<00:00, 15.85it/s]\n",
      "Epoch 450, Test Loss: 0.0035726602654904127, Train Loss: 0.004259367007762194: 100%|██████████| 500/500 [00:30<00:00, 16.17it/s] \n",
      "Epoch 450, Test Loss: 0.013457849621772766, Train Loss: 0.013394169509410858: 100%|██████████| 500/500 [00:30<00:00, 16.60it/s]\n",
      "Epoch 950, Test Loss: 0.0019181349780410528, Train Loss: 0.001857992960140109: 100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n",
      "Epoch 950, Test Loss: 0.004360455088317394, Train Loss: 0.002074990887194872: 100%|██████████| 1000/1000 [01:00<00:00, 16.65it/s]\n",
      "Epoch 950, Test Loss: 0.0029448773711919785, Train Loss: 0.001773992320522666: 100%|██████████| 1000/1000 [01:01<00:00, 16.24it/s]\n",
      "Epoch 950, Test Loss: 0.00303635373711586, Train Loss: 0.002693893387913704: 100%|██████████| 1000/1000 [00:59<00:00, 16.72it/s]  \n",
      "Epoch 950, Test Loss: 0.0031230223830789328, Train Loss: 0.0019002167973667383: 100%|██████████| 1000/1000 [00:59<00:00, 16.67it/s]\n",
      "Epoch 950, Test Loss: 0.003919560462236404, Train Loss: 0.0041666156612336636: 100%|██████████| 1000/1000 [00:56<00:00, 17.80it/s]\n",
      "Epoch 950, Test Loss: 0.004495608154684305, Train Loss: 0.0021163341589272022: 100%|██████████| 1000/1000 [00:56<00:00, 17.73it/s]\n",
      "Epoch 950, Test Loss: 0.016693538054823875, Train Loss: 0.012576867826282978: 100%|██████████| 1000/1000 [00:57<00:00, 17.39it/s]\n",
      "Epoch 950, Test Loss: 0.002955211326479912, Train Loss: 0.002808372024446726: 100%|██████████| 1000/1000 [00:56<00:00, 17.72it/s] \n",
      "Epoch 950, Test Loss: 0.013616257347166538, Train Loss: 0.01352574210613966: 100%|██████████| 1000/1000 [00:57<00:00, 17.39it/s]\n",
      "Epoch 950, Test Loss: 0.012172263115644455, Train Loss: 0.013712682761251926: 100%|██████████| 1000/1000 [00:56<00:00, 17.86it/s]\n",
      "Epoch 950, Test Loss: 0.018786897882819176, Train Loss: 0.01796998269855976: 100%|██████████| 1000/1000 [00:57<00:00, 17.36it/s]\n",
      "Epoch 950, Test Loss: 0.01694815792143345, Train Loss: 0.01335853897035122: 100%|██████████| 1000/1000 [00:56<00:00, 17.62it/s] \n",
      "Epoch 950, Test Loss: 0.011366418562829494, Train Loss: 0.01517318096011877: 100%|██████████| 1000/1000 [00:56<00:00, 17.60it/s]\n",
      "Epoch 950, Test Loss: 0.013654951006174088, Train Loss: 0.013561833649873734: 100%|██████████| 1000/1000 [00:56<00:00, 17.63it/s]\n",
      "Epoch 1450, Test Loss: 0.0019787452183663845, Train Loss: 0.0016727049369364977: 100%|██████████| 1500/1500 [01:23<00:00, 17.95it/s]\n",
      "Epoch 1450, Test Loss: 0.009214472956955433, Train Loss: 0.0017819313798099756: 100%|██████████| 1500/1500 [01:21<00:00, 18.34it/s]\n",
      "Epoch 1450, Test Loss: 0.002742155222222209, Train Loss: 0.0018093035323545337: 100%|██████████| 1500/1500 [01:22<00:00, 18.13it/s] \n",
      "Epoch 1450, Test Loss: 0.0018205203814432025, Train Loss: 0.0018049182835966349: 100%|██████████| 1500/1500 [01:22<00:00, 18.17it/s]\n",
      "Epoch 1450, Test Loss: 0.0034394797403365374, Train Loss: 0.0019028453389182687: 100%|██████████| 1500/1500 [01:21<00:00, 18.45it/s]\n",
      "Epoch 1450, Test Loss: 0.0031719247344881296, Train Loss: 0.004102902486920357: 100%|██████████| 1500/1500 [01:25<00:00, 17.62it/s]\n",
      "Epoch 1450, Test Loss: 0.013559993356466293, Train Loss: 0.013364201411604881: 100%|██████████| 1500/1500 [01:23<00:00, 18.07it/s]\n",
      "Epoch 1450, Test Loss: 0.002866727765649557, Train Loss: 0.0029582278802990913: 100%|██████████| 1500/1500 [01:22<00:00, 18.25it/s] \n",
      "Epoch 1450, Test Loss: 0.0019297684775665402, Train Loss: 0.0026365311350673437: 100%|██████████| 1500/1500 [01:22<00:00, 18.28it/s]\n",
      "Epoch 1450, Test Loss: 0.002544722519814968, Train Loss: 0.0017871479503810406: 100%|██████████| 1500/1500 [01:22<00:00, 18.15it/s] \n",
      "Epoch 1450, Test Loss: 0.012776527553796768, Train Loss: 0.01444774866104126: 100%|██████████| 1500/1500 [01:22<00:00, 18.23it/s] \n",
      "Epoch 1450, Test Loss: 0.013782297261059284, Train Loss: 0.01373052503913641: 100%|██████████| 1500/1500 [01:23<00:00, 18.04it/s] \n",
      "Epoch 1450, Test Loss: 0.016759507358074188, Train Loss: 0.012578758411109447: 100%|██████████| 1500/1500 [01:21<00:00, 18.34it/s]\n",
      "Epoch 1450, Test Loss: 0.01138911210000515, Train Loss: 0.015214296989142895: 100%|██████████| 1500/1500 [01:24<00:00, 17.67it/s] \n",
      "Epoch 1450, Test Loss: 0.014193268492817879, Train Loss: 0.014175083488225937: 100%|██████████| 1500/1500 [01:23<00:00, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure: {1: 8}, Epochs: 1500, Learning Rate: 0.1\n",
      "Structure: {1: 4}, Epochs: 1500, Learning Rate: 0.1\n",
      "Structure: {1: 4, 2: 4}, Epochs: 500, Learning Rate: 0.1\n",
      "Structure: {1: 8, 2: 4}, Epochs: 100, Learning Rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "structures = [\n",
    "    {1: 8},\n",
    "    {1: 4},\n",
    "    {1: 4, 2: 4},\n",
    "    {1: 8, 2: 4}\n",
    "]\n",
    "\n",
    "lrs = []\n",
    "epochs = []\n",
    "\n",
    "for structure in structures:\n",
    "    best_epochs, best_lr = grid_search(structure)\n",
    "    lrs.append(best_lr)\n",
    "    epochs.append(best_epochs)\n",
    "    \n",
    "for i, structure in enumerate(structures):\n",
    "    print(f\"Structure: {structure}, Epochs: {epochs[i]}, Learning Rate: {lrs[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:45:38.320013200Z",
     "start_time": "2023-12-13T14:01:28.610703700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 950, Test Loss: 0.002307337708771229, Train Loss: 0.0021706109400838614: 100%|██████████| 1000/1000 [00:50<00:00, 19.77it/s]\n",
      "Epoch 950, Test Loss: 0.003010334214195609, Train Loss: 0.003253083908930421: 100%|██████████| 1000/1000 [00:45<00:00, 21.98it/s] \n",
      "Epoch 950, Test Loss: 0.002046388341113925, Train Loss: 0.001738176797516644: 100%|██████████| 1000/1000 [00:54<00:00, 18.31it/s] \n",
      "Epoch 950, Test Loss: 0.00218328763730824, Train Loss: 0.0019523411756381392: 100%|██████████| 1000/1000 [01:04<00:00, 15.61it/s] \n",
      "Epoch 950, Test Loss: 0.005286112427711487, Train Loss: 0.003327341051772237: 100%|██████████| 1000/1000 [00:54<00:00, 18.47it/s]\n",
      "Epoch 950, Test Loss: 0.007844577543437481, Train Loss: 0.0026930051390081644: 100%|██████████| 1000/1000 [00:58<00:00, 17.09it/s]\n",
      "Epoch 950, Test Loss: 0.004448226187378168, Train Loss: 0.002190494444221258: 100%|██████████| 1000/1000 [01:09<00:00, 14.29it/s] \n",
      "Epoch 950, Test Loss: 0.004263903945684433, Train Loss: 0.0034298335667699575: 100%|██████████| 1000/1000 [01:07<00:00, 14.87it/s]\n",
      "Epoch 950, Test Loss: 0.0033418391831219196, Train Loss: 0.002844426082447171: 100%|██████████| 1000/1000 [01:00<00:00, 16.66it/s]\n",
      "Epoch 950, Test Loss: 0.0037351311184465885, Train Loss: 0.0033120564185082912: 100%|██████████| 1000/1000 [00:56<00:00, 17.69it/s]\n",
      "Epoch 950, Test Loss: 0.004062092397361994, Train Loss: 0.0035281265154480934: 100%|██████████| 1000/1000 [01:07<00:00, 14.72it/s]\n",
      "Epoch 950, Test Loss: 0.002931437687948346, Train Loss: 0.002368524204939604: 100%|██████████| 1000/1000 [01:06<00:00, 15.13it/s] \n",
      "Epoch 950, Test Loss: 0.001986046088859439, Train Loss: 0.002338876947760582: 100%|██████████| 1000/1000 [00:56<00:00, 17.80it/s] \n",
      "Epoch 950, Test Loss: 0.0031709126196801662, Train Loss: 0.0033910584170371294: 100%|██████████| 1000/1000 [00:55<00:00, 17.95it/s]\n",
      "Epoch 950, Test Loss: 0.0031648112926632166, Train Loss: 0.004244594369083643: 100%|██████████| 1000/1000 [01:06<00:00, 15.08it/s]\n",
      "Epoch 950, Test Loss: 0.00218162196688354, Train Loss: 0.0017760064220055938: 100%|██████████| 1000/1000 [01:07<00:00, 14.85it/s] \n",
      "Epoch 950, Test Loss: 0.007594907656311989, Train Loss: 0.0025731874629855156: 100%|██████████| 1000/1000 [00:57<00:00, 17.51it/s]\n",
      "Epoch 950, Test Loss: 0.006059471517801285, Train Loss: 0.0022593208122998476: 100%|██████████| 1000/1000 [00:56<00:00, 17.78it/s]\n",
      "Epoch 950, Test Loss: 0.005088848527520895, Train Loss: 0.0025441995821893215: 100%|██████████| 1000/1000 [01:06<00:00, 15.08it/s]\n",
      "Epoch 950, Test Loss: 0.003345766570419073, Train Loss: 0.0020499874372035265: 100%|██████████| 1000/1000 [01:07<00:00, 14.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from itertools import product\n",
    "\n",
    "epoch_lengts = [100, 500, 1000, 1500]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "params = product(epoch_lengts, learning_rates)\n",
    "\n",
    "r2_mlp8 = []\n",
    "rmse_mlp8 = []\n",
    "\n",
    "r2_mlp4 = []\n",
    "rmse_mlp4 = []\n",
    "\n",
    "r2_mlp44 = []\n",
    "rmse_mlp44 = []\n",
    "\n",
    "r2_mlp84 = []\n",
    "rmse_mlp84 = []\n",
    "\n",
    "\n",
    "for df_train, df_test in folds:\n",
    "    X_train = torch.tensor(df_train.iloc[:, :-1].values, dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(df_train.iloc[:, -1].values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "    X_test = torch.tensor(df_test.iloc[:, :-1].values, dtype=torch.float32, device=device)\n",
    "    y_test = torch.tensor(df_test.iloc[:, -1].values, dtype=torch.float32, device=device).view(-1, 1)\n",
    "    \n",
    "    y_test_numpy = df_test.iloc[:, -1].values\n",
    "    \n",
    "    model = MLP({1: 8}, X_train.shape[1]).to(device)\n",
    "    train(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    prediction = model(X_test).cpu().detach().numpy()\n",
    "    rmse_mlp8.append(np.sqrt(mean_squared_error(y_test_numpy, prediction)))\n",
    "    r2_mlp8.append(r2_score(y_test_numpy, prediction))\n",
    "    \n",
    "    model = MLP({1: 4}, X_train.shape[1]).to(device)\n",
    "    train(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    prediction = model(X_test).cpu().detach().numpy()\n",
    "    rmse_mlp4.append(np.sqrt(mean_squared_error(y_test_numpy, prediction)))\n",
    "    r2_mlp4.append(r2_score(y_test_numpy, prediction))\n",
    "    \n",
    "    model = MLP({1: 4, 2: 4}, X_train.shape[1]).to(device)\n",
    "    train(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    prediction = model(X_test).cpu().detach().numpy()\n",
    "    rmse_mlp44.append(np.sqrt(mean_squared_error(y_test_numpy, prediction)))\n",
    "    r2_mlp44.append(r2_score(y_test_numpy, prediction))\n",
    "    \n",
    "    model = MLP({1: 8, 2: 4}, X_train.shape[1]).to(device)\n",
    "    train(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    prediction = model(X_test).cpu().detach().numpy()\n",
    "    rmse_mlp84.append(np.sqrt(mean_squared_error(y_test_numpy, prediction)))\n",
    "    r2_mlp84.append(r2_score(y_test_numpy, prediction))\n",
    "    \n",
    "r2_mlp8_mean = np.mean(r2_mlp8)\n",
    "rmse_mlp8_mean = np.mean(rmse_mlp8)\n",
    "\n",
    "r2_mlp4_mean = np.mean(r2_mlp4)\n",
    "rmse_mlp4_mean = np.mean(rmse_mlp4)\n",
    "\n",
    "r2_mlp44_mean = np.mean(r2_mlp44)\n",
    "rmse_mlp44_mean = np.mean(rmse_mlp44)\n",
    "\n",
    "r2_mlp84_mean = np.mean(r2_mlp84)\n",
    "rmse_mlp84_mean = np.mean(rmse_mlp84)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:16:59.117309500Z",
     "start_time": "2023-12-13T12:56:50.645331600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Combining Technical Analysis Indicators with 2D CNN on Bitcoin Direction Prediction\n",
    "\n",
    "In this problem, we will focus on adding technical analysis indicators to original series, which will help us convert it into 2D Image. We will use the following technical indicators from TA-Lib library in Python: MACD, RSI, CMO, MOM, Bollinger Bands, SMA. In general, technical analysis indicators are financial indicators which give trades a guidance about the market. Our train period is 2021-2022 and test period will be 2022-2023.\n",
    "\n",
    "We will use historical 6 days closing price, build up 6x6 image by calculating technical indicators, and predict the direction for the next day (whether the price will be up or down). We will use a single convolutional layer followed by Fully Connected Layer where kernel size=(2,2) can be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T18:38:58.968796600Z",
     "start_time": "2023-12-16T18:38:58.914784600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                   Close\nDate                    \n2021-01-01  29374.152344\n2021-01-02  32127.267578\n2021-01-03  32782.023438\n2021-01-04  31971.914063\n2021-01-05  33992.429688\n...                  ...\n2022-12-27  16717.173828\n2022-12-28  16552.572266\n2022-12-29  16642.341797\n2022-12-30  16602.585938\n2022-12-31  16547.496094\n\n[730 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Close</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-01-01</th>\n      <td>29374.152344</td>\n    </tr>\n    <tr>\n      <th>2021-01-02</th>\n      <td>32127.267578</td>\n    </tr>\n    <tr>\n      <th>2021-01-03</th>\n      <td>32782.023438</td>\n    </tr>\n    <tr>\n      <th>2021-01-04</th>\n      <td>31971.914063</td>\n    </tr>\n    <tr>\n      <th>2021-01-05</th>\n      <td>33992.429688</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-12-27</th>\n      <td>16717.173828</td>\n    </tr>\n    <tr>\n      <th>2022-12-28</th>\n      <td>16552.572266</td>\n    </tr>\n    <tr>\n      <th>2022-12-29</th>\n      <td>16642.341797</td>\n    </tr>\n    <tr>\n      <th>2022-12-30</th>\n      <td>16602.585938</td>\n    </tr>\n    <tr>\n      <th>2022-12-31</th>\n      <td>16547.496094</td>\n    </tr>\n  </tbody>\n</table>\n<p>730 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution 3\n",
    "df_test = pd.read_csv('btc_test.csv')\n",
    "df_train = pd.read_csv('btc_train.csv')\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df = df.dropna()\n",
    "df = df[[\"Date\", \"Close\"]]\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "                  MACD        RSI        CMO          MOM        BBANDS  \\\nDate                                                                      \n2021-02-03 -226.422720  62.098634  24.197269  5182.710938  37925.235103   \n2021-02-04  -15.039748  60.319527  20.639054  4559.673828  38809.440003   \n2021-02-05  247.926798  62.875123  25.750246  5574.458985  39594.868785   \n2021-02-06  540.609933  65.103681  30.207362  8833.464844  39964.391466   \n2021-02-07  734.836297  63.771135  27.542271  5437.343750  39879.156250   \n...                ...        ...        ...          ...           ...   \n2022-12-27 -103.142239  44.860801 -10.278397   -77.917969  16958.022581   \n2022-12-28 -120.315843  42.121070 -15.757859  -205.404297  17034.413436   \n2022-12-29 -125.238723  44.125248 -11.749504   202.662109  16999.776595   \n2022-12-30 -130.839866  43.408369 -13.183263  -303.718750  16943.487726   \n2022-12-31 -138.131805  42.380864 -15.238272  -270.039062  16738.171015   \n\n                     SMA  direction  \nDate                                 \n2021-02-03  35262.511524          0  \n2021-02-04  35360.299414          1  \n2021-02-05  35404.297591          1  \n2021-02-06  35400.796550          0  \n2021-02-07  35337.657617          1  \n...                  ...        ...  \n2022-12-27  16964.124935          0  \n2022-12-28  16975.299935          1  \n2022-12-29  16981.878581          0  \n2022-12-30  16963.012565          0  \n2022-12-31  16949.024675          0  \n\n[697 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MACD</th>\n      <th>RSI</th>\n      <th>CMO</th>\n      <th>MOM</th>\n      <th>BBANDS</th>\n      <th>SMA</th>\n      <th>direction</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-02-03</th>\n      <td>-226.422720</td>\n      <td>62.098634</td>\n      <td>24.197269</td>\n      <td>5182.710938</td>\n      <td>37925.235103</td>\n      <td>35262.511524</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-02-04</th>\n      <td>-15.039748</td>\n      <td>60.319527</td>\n      <td>20.639054</td>\n      <td>4559.673828</td>\n      <td>38809.440003</td>\n      <td>35360.299414</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-02-05</th>\n      <td>247.926798</td>\n      <td>62.875123</td>\n      <td>25.750246</td>\n      <td>5574.458985</td>\n      <td>39594.868785</td>\n      <td>35404.297591</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-02-06</th>\n      <td>540.609933</td>\n      <td>65.103681</td>\n      <td>30.207362</td>\n      <td>8833.464844</td>\n      <td>39964.391466</td>\n      <td>35400.796550</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-02-07</th>\n      <td>734.836297</td>\n      <td>63.771135</td>\n      <td>27.542271</td>\n      <td>5437.343750</td>\n      <td>39879.156250</td>\n      <td>35337.657617</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-12-27</th>\n      <td>-103.142239</td>\n      <td>44.860801</td>\n      <td>-10.278397</td>\n      <td>-77.917969</td>\n      <td>16958.022581</td>\n      <td>16964.124935</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2022-12-28</th>\n      <td>-120.315843</td>\n      <td>42.121070</td>\n      <td>-15.757859</td>\n      <td>-205.404297</td>\n      <td>17034.413436</td>\n      <td>16975.299935</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2022-12-29</th>\n      <td>-125.238723</td>\n      <td>44.125248</td>\n      <td>-11.749504</td>\n      <td>202.662109</td>\n      <td>16999.776595</td>\n      <td>16981.878581</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2022-12-30</th>\n      <td>-130.839866</td>\n      <td>43.408369</td>\n      <td>-13.183263</td>\n      <td>-303.718750</td>\n      <td>16943.487726</td>\n      <td>16963.012565</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2022-12-31</th>\n      <td>-138.131805</td>\n      <td>42.380864</td>\n      <td>-15.238272</td>\n      <td>-270.039062</td>\n      <td>16738.171015</td>\n      <td>16949.024675</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>697 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import talib\n",
    "df[\"MACD\"] = talib.MACD(df[\"Close\"])[0]\n",
    "df[\"RSI\"] = talib.RSI(df[\"Close\"])\n",
    "df[\"CMO\"] = talib.CMO(df[\"Close\"])\n",
    "df[\"MOM\"] = talib.MOM(df[\"Close\"])\n",
    "df[\"BBANDS\"] = talib.BBANDS(df[\"Close\"])[0]\n",
    "df[\"SMA\"] = talib.SMA(df[\"Close\"])\n",
    "df[\"next_close\"] = df.loc[:, \"Close\"].shift(-1)\n",
    "df[\"direction\"] = (df[\"next_close\"] > df[\"Close\"]).astype(int)\n",
    "\n",
    "df.drop(labels=[\"Close\", \"next_close\"], axis = 1, inplace=True)\n",
    "df = df.dropna()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:38:59.192385300Z",
     "start_time": "2023-12-16T18:38:59.176784700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "                MACD       RSI       CMO       MOM    BBANDS       SMA  \\\nDate                                                                     \n2021-02-03  0.479715  0.694851  0.694851  0.741884  0.402851  0.402763   \n2021-02-04  0.500722  0.665358  0.665358  0.724384  0.419576  0.404884   \n2021-02-05  0.526856  0.707724  0.707724  0.752888  0.434433  0.405839   \n2021-02-06  0.555943  0.744669  0.744669  0.844428  0.441423  0.405763   \n2021-02-07  0.575245  0.722578  0.722578  0.749036  0.439810  0.404393   \n...              ...       ...       ...       ...       ...       ...   \n2022-12-27  0.491967  0.409085  0.409085  0.594122  0.006249  0.005795   \n2022-12-28  0.490260  0.363666  0.363666  0.590541  0.007694  0.006038   \n2022-12-29  0.489771  0.396891  0.396891  0.602003  0.007039  0.006180   \n2022-12-30  0.489214  0.385006  0.385006  0.587780  0.005974  0.005771   \n2022-12-31  0.488490  0.367972  0.367972  0.588726  0.002090  0.005468   \n\n            direction  \nDate                   \n2021-02-03        0.0  \n2021-02-04        1.0  \n2021-02-05        1.0  \n2021-02-06        0.0  \n2021-02-07        1.0  \n...               ...  \n2022-12-27        0.0  \n2022-12-28        1.0  \n2022-12-29        0.0  \n2022-12-30        0.0  \n2022-12-31        0.0  \n\n[697 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MACD</th>\n      <th>RSI</th>\n      <th>CMO</th>\n      <th>MOM</th>\n      <th>BBANDS</th>\n      <th>SMA</th>\n      <th>direction</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-02-03</th>\n      <td>0.479715</td>\n      <td>0.694851</td>\n      <td>0.694851</td>\n      <td>0.741884</td>\n      <td>0.402851</td>\n      <td>0.402763</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2021-02-04</th>\n      <td>0.500722</td>\n      <td>0.665358</td>\n      <td>0.665358</td>\n      <td>0.724384</td>\n      <td>0.419576</td>\n      <td>0.404884</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2021-02-05</th>\n      <td>0.526856</td>\n      <td>0.707724</td>\n      <td>0.707724</td>\n      <td>0.752888</td>\n      <td>0.434433</td>\n      <td>0.405839</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2021-02-06</th>\n      <td>0.555943</td>\n      <td>0.744669</td>\n      <td>0.744669</td>\n      <td>0.844428</td>\n      <td>0.441423</td>\n      <td>0.405763</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2021-02-07</th>\n      <td>0.575245</td>\n      <td>0.722578</td>\n      <td>0.722578</td>\n      <td>0.749036</td>\n      <td>0.439810</td>\n      <td>0.404393</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-12-27</th>\n      <td>0.491967</td>\n      <td>0.409085</td>\n      <td>0.409085</td>\n      <td>0.594122</td>\n      <td>0.006249</td>\n      <td>0.005795</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2022-12-28</th>\n      <td>0.490260</td>\n      <td>0.363666</td>\n      <td>0.363666</td>\n      <td>0.590541</td>\n      <td>0.007694</td>\n      <td>0.006038</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2022-12-29</th>\n      <td>0.489771</td>\n      <td>0.396891</td>\n      <td>0.396891</td>\n      <td>0.602003</td>\n      <td>0.007039</td>\n      <td>0.006180</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2022-12-30</th>\n      <td>0.489214</td>\n      <td>0.385006</td>\n      <td>0.385006</td>\n      <td>0.587780</td>\n      <td>0.005974</td>\n      <td>0.005771</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2022-12-31</th>\n      <td>0.488490</td>\n      <td>0.367972</td>\n      <td>0.367972</td>\n      <td>0.588726</td>\n      <td>0.002090</td>\n      <td>0.005468</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>697 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame(columns=df.columns, index=df.index, data=MinMaxScaler().fit_transform(df))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:02.533799100Z",
     "start_time": "2023-12-16T18:39:02.484787800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "df_test = df.loc[\"2022-01-01\":\"2023-01-01\"]\n",
    "df_train = df.drop(df_test.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:03.868787500Z",
     "start_time": "2023-12-16T18:39:03.850783100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "def get_formatted_X_y(_df: pd.DataFrame, lag_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(_df) - lag_length):\n",
    "        endi = i + lag_length\n",
    "        \n",
    "        X.append(_df.iloc[i:endi, :-1].values)\n",
    "        y.append(_df.iloc[endi - 1, -1])\n",
    "    \n",
    "    return np.array(X).reshape(-1, 1, lag_length, 6), np.array(y).reshape(-1, 1)\n",
    "\n",
    "X_train, y_train = get_formatted_X_y(df_train, 6)\n",
    "X_test, y_test = get_formatted_X_y(df_test, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:08.259784600Z",
     "start_time": "2023-12-16T18:39:08.181584300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(1, 1, kernel_size=(2, 2)))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(25, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:08.701301Z",
     "start_time": "2023-12-16T18:39:08.678266400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:09.433265300Z",
     "start_time": "2023-12-16T18:39:09.418753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.]], device='cuda:0')"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:39:10.161540300Z",
     "start_time": "2023-12-16T18:39:10.094987400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train(_model: CNN, X_train: torch.Tensor, y_train: torch.Tensor, X_test: torch.Tensor, y_test: torch.Tensor, learning_rate, epochs):\n",
    "    batch_size = 32\n",
    "    optimizer = torch.optim.Adam(_model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    epoch_bar = tqdm(range(epochs))\n",
    "    for epoch in epoch_bar:\n",
    "        for X, y in dataloader:\n",
    "            pred = _model(X)\n",
    "            loss = loss_function(pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            acc = torch.sum((torch.round(_model(X_test)) == y_test).float()) / len(y_test)\n",
    "            \n",
    "            epoch_bar.set_description(f\"Epoch {epoch}, Test Loss: {loss_function(_model(X_test), y_test)}, Train Loss: {loss_function(_model(X_train), y_train)}, Accuracy:\"\n",
    "                                      f\"{acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:43:24.097790300Z",
     "start_time": "2023-12-16T18:43:24.078786500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 230, Test Loss: 0.25527423620224, Train Loss: 0.2492552399635315, Accuracy:0.470752090215683:  23%|██▎       | 232/1000 [00:04<00:15, 49.60it/s]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[228], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m cnn \u001B[38;5;241m=\u001B[39m CNN()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[226], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(_model, X_train, y_train, X_test, y_test, learning_rate, epochs)\u001B[0m\n\u001B[0;32m     15\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_function(pred, y)\n\u001B[0;32m     17\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 18\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS540HW3\\venv\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS540HW3\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cnn = CNN().to(device)\n",
    "train(cnn, X_train, y_train, X_test, y_test, 1e-4, 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:43:50.444844500Z",
     "start_time": "2023-12-16T18:43:45.716300700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Multivariate LSTM for Predicting EPS (Earnings per Share) over Company Fundamentals\n",
    "\n",
    "In this problem, we will focus on predicting Earnings Per Share (EPS) by jointly modeling historical fundamentals where fundamentals for multiple companies in \"fundamentals.csv\" file for each year. Number of latent dimension of LSTM can be [5, 10, 30] and the best one can be determined by hyperparameter search. On the other hand, learning rate and number of epochs should be carefully tuned. Our evaluation metric will be MAPE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T10:42:25.432186300Z",
     "start_time": "2023-12-13T10:42:25.419184100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Solution 4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
